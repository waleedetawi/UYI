{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ea631-aee7-43fc-9cbd-53251c883a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automated grants ( specifically for start-ups in the pre-seed/ideation/mvp stage in Jordan or internationally if applicable) \n",
    "#Your first task for this project will be implementing a code that searches and gathers data about said grants, saves in a notepad the date of the search\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaea02-f383-49b9-8b1c-7c19f1812ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ChromeDriver Path\n",
    "chrome_driver_path = r\"C:\\Users\\Syndictech\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no browser window)\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "\n",
    "# Setting up the webdriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Search for grants specific to startups in Jordan or internationally\n",
    "search_query = \"grants for startups in pre-seed ideation MVP stage in Jordan\"\n",
    "apply_website_filter = input(\"Do you want to find in a specific website (yes/no)? \").strip().lower()\n",
    "\n",
    "if apply_website_filter == 'yes':\n",
    "    website_filter = input(\"Enter the website name (ex: linkedin): \").strip()\n",
    "    search_query += \" site:\" + website_filter\n",
    "    print(\"New search query:\", search_query)\n",
    "\n",
    "# User input for number of links to process\n",
    "num_links_to_process = int(input(\"Enter the number of links to process: \"))\n",
    "\n",
    "def apply_filter(href):\n",
    "    \"\"\"\n",
    "    Filter links based on certain criteria to avoid unwanted or irrelevant content.\n",
    "    \n",
    "    Args:\n",
    "    href (str): The URL to be checked.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the link is considered valid, False otherwise.\n",
    "    \"\"\"\n",
    "    unwanted_keywords = ['products', 'ads', 'jobs', 'signup']\n",
    "    return (href and not href.startswith('/search') and 'google.com' not in href and\n",
    "            all(keyword not in href for keyword in unwanted_keywords))\n",
    "\n",
    "def save_to_notepad(data, file_name=\"grant_search_results.txt\"):\n",
    "    \"\"\"\n",
    "    Save the provided data to a text file.\n",
    "\n",
    "    Args:\n",
    "    data (str): The data to be saved.\n",
    "    file_name (str): The name of the file to save the data in. Default is \"grant_search_results.txt\".\n",
    "    \"\"\"\n",
    "    with open(file_name, 'a', encoding='utf-8') as file:\n",
    "        file.write(data + \"\\n\")\n",
    "\n",
    "def extract_links_and_info(page_number):\n",
    "    \"\"\"\n",
    "    Extracts links from the current page, visits each link, and saves the title and content to a file.\n",
    "\n",
    "    Args:\n",
    "    page_number (int): The page number from which links are being extracted.\n",
    "    \"\"\"\n",
    "    links = driver.find_elements(By.XPATH, '//a[@href]')\n",
    "    processed_count = 0\n",
    "\n",
    "    for link in links:\n",
    "        if processed_count >= num_links_to_process:\n",
    "            break\n",
    "\n",
    "        href = link.get_attribute('href')\n",
    "        if apply_filter(href):\n",
    "            try:\n",
    "                # Open the link in a new tab\n",
    "                driver.execute_script(f\"window.open('{href}', '_blank');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "                # Extract the entire HTML content\n",
    "                page_html = driver.page_source\n",
    "\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(page_html, 'lxml')\n",
    "\n",
    "                # Extract and format the title\n",
    "                title = soup.title.string if soup.title else 'No title found'\n",
    "\n",
    "                # Extract and format the text content\n",
    "                paragraphs = soup.find_all('p')\n",
    "                text_content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "                # Prepare data to be saved\n",
    "                data_to_save = f\"Link: {href}\\nTitle: {title}\\nText Content:\\n{text_content}\\n{'='*80}\\n\"\n",
    "\n",
    "                # Save to notepad\n",
    "                save_to_notepad(data_to_save)\n",
    "\n",
    "                # Close the tab and switch back\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not extract info from {href}: {e}\")\n",
    "\n",
    "    print(f\"\\nPage {page_number} links and info extracted.\\n\")\n",
    "\n",
    "# Searching\n",
    "driver.get('https://www.google.com/')\n",
    "search_box = driver.find_element(By.NAME, 'q')\n",
    "search_box.send_keys(search_query)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load to take the links\n",
    "time.sleep(6)\n",
    "\n",
    "# Save the date of the search\n",
    "current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "save_to_notepad(f\"Search Date: {current_date}\\nQuery: {search_query}\\n{'='*80}\")\n",
    "\n",
    "# Page number counter\n",
    "page_number = 1\n",
    "\n",
    "# Extract links and info from the first page\n",
    "extract_links_and_info(page_number)\n",
    "\n",
    "# Going to the next page till the last page or until enough links are processed\n",
    "while True:\n",
    "    try:\n",
    "        if num_links_to_process <= 0:\n",
    "            break\n",
    "        \n",
    "        # Check if there is a next button\n",
    "        next_button = driver.find_elements(By.ID, 'pnnext')  # We used the ID to click on the next button\n",
    "        if next_button:\n",
    "            next_button[0].click()\n",
    "        else:\n",
    "            print(\"You're at the last page; 'Next' button not found.\")\n",
    "            break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        # Extract links and info from the next page\n",
    "        extract_links_and_info(page_number)\n",
    "    except Exception as e:\n",
    "        print(\"No more pages:\", e)\n",
    "        break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Results saved in 'grant_search_results.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
