{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f864efda-f55f-42d7-9027-ae178f9f3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\syndictech\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dba3b38-bbf7-4091-9e4d-748c9ef92df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Available AI Companies: ['OpenAI', 'Google DeepMind', 'NVIDIA', 'Microsoft', 'IBM', 'Amazon Web Services', 'Facebook AI Research']\n",
      "Enter company names separated by commas (or type 'all' to search all):  OpenAI\n",
      "Enter the number of links to process:  3\n",
      "Do you want to scrape LinkedIn for AI company news? (yes/no):  yes\n",
      "Enter your LinkedIn username:  waleedetawi111@gmail.com\n",
      "Enter your LinkedIn password:  W@leed3tawi#2004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: OpenAI\n",
      "LinkedIn scraping complete for OpenAI. Results saved in 'linkedin_results.xlsx'.\n",
      "Processed: Several Top News Sites Shun OpenAI's SearchGPT Search Engine - Business Insider\n",
      "Processed: OpenAI Blocks Iranian Influence Operation Using ChatGPT for U.S. Election Propaganda\n",
      "Processed: You wanted to try OpenAI's SearchGPT? It's time to look for AI alternatives | ZDNET\n",
      "Results saved in 'ai_news_results.txt' and 'linkedin_results.csv' if applicable.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ChromeDriver Path\n",
    "chrome_driver_path = r\"C:\\Users\\Syndictech\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "# chrome_options.add_argument(\"--headless\")  # Keep this commented to see the browser in action\n",
    "\n",
    "# Setting up the webdriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# List of AI companies\n",
    "ai_companies = ['OpenAI', 'Google DeepMind', 'NVIDIA', 'Microsoft', 'IBM', 'Amazon Web Services', 'Facebook AI Research']\n",
    "\n",
    "# User input for company names\n",
    "user_input = input(f\"Available AI Companies: {ai_companies}\\nEnter company names separated by commas (or type 'all' to search all): \")\n",
    "if user_input.strip().lower() == 'all':\n",
    "    companies_to_search = ai_companies\n",
    "else:\n",
    "    companies_to_search = [company.strip() for company in user_input.split(',') if company.strip() in ai_companies]\n",
    "\n",
    "# User input for number of links to process\n",
    "num_links_to_process = int(input(\"Enter the number of links to process: \"))\n",
    "\n",
    "scrape_linkedin = input(\"Do you want to scrape LinkedIn for AI company news? (yes/no): \").strip().lower() == 'yes'\n",
    "\n",
    "if scrape_linkedin:\n",
    "    linkedin_username = input(\"Enter your LinkedIn username: \")\n",
    "    linkedin_password = input(\"Enter your LinkedIn password: \")\n",
    "    \n",
    "    def linkedin_login(username, password):\n",
    "        linkedin_login_url = \"https://www.linkedin.com/login\"\n",
    "        driver.get(linkedin_login_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Locate the username and password fields and enter the credentials\n",
    "        username_input = driver.find_element(By.ID, \"username\")\n",
    "        password_input = driver.find_element(By.ID, \"password\")\n",
    "        \n",
    "        username_input.send_keys(username)\n",
    "        password_input.send_keys(password)\n",
    "        \n",
    "        # Click the login button\n",
    "        login_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "        login_button.click()\n",
    "        \n",
    "        time.sleep(5)  \n",
    "\n",
    "    linkedin_login(linkedin_username, linkedin_password)\n",
    "\n",
    "def save_to_notepad(data, file_name=\"ai_news_results.txt\"):\n",
    "    with open(file_name, 'a', encoding='utf-8') as file:\n",
    "        file.write(data + \"\\n\")\n",
    "\n",
    "def extract_article_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    title = soup.title.string if soup.title else 'No title found'\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    # Combine text content and filter short or irrelevant content\n",
    "    text_content = \"\\n\".join(p.get_text() for p in paragraphs if len(p.get_text()) > 50)\n",
    "    \n",
    "    if len(text_content.split()) < 300:  # Check if the content is substantial enough (300 words)\n",
    "        return None, None\n",
    "    \n",
    "    return title, text_content\n",
    "\n",
    "def extract_links_and_info(search_query):\n",
    "    search_url = f\"https://www.google.com/search?q={search_query.replace(' ', '+')}&tbm=nws\"\n",
    "    processed_count = 0\n",
    "    unique_links = set()  # To keep track of processed links\n",
    "    page_number = 0\n",
    "    \n",
    "    while processed_count < num_links_to_process:\n",
    "        driver.get(search_url + f\"&start={page_number * 10}\")  # Go to the next page if necessary\n",
    "        time.sleep(2)  \n",
    "\n",
    "        # Extract the news links and process them\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            \n",
    "            # Skip links that are not actual URLs (e.g., Google internal links)\n",
    "            if href.startswith('/'):\n",
    "                continue\n",
    "            \n",
    "            if 'url?q=' in href:  # Google search result links are often in the form \"url?q=...\"\n",
    "                href = href.split('url?q=')[1].split('&')[0]\n",
    "\n",
    "            # Skip links that are not from news domains or look like product pages\n",
    "            if not any(domain in href for domain in ['news', 'blog', 'article']):\n",
    "                continue\n",
    "\n",
    "            # Ensure the link is valid and hasn't been processed\n",
    "            if 'https://' in href and href not in unique_links:\n",
    "                unique_links.add(href)\n",
    "\n",
    "                try:\n",
    "                    title, content = extract_article_info(href)\n",
    "\n",
    "                    # If content is None, skip this link\n",
    "                    if title is None or content is None:\n",
    "                        continue\n",
    "\n",
    "                    data_to_save = f\"Link: {href}\\nTitle: {title}\\nText Content:\\n{content}\\n{'='*80}\\n\"\n",
    "                    save_to_notepad(data_to_save)\n",
    "                    print(f\"Processed: {title}\")\n",
    "                    processed_count += 1\n",
    "\n",
    "                    if processed_count >= num_links_to_process:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing link: {e}\")\n",
    "\n",
    "        if processed_count >= num_links_to_process:\n",
    "            break\n",
    "\n",
    "        page_number += 1  # Increment to the next page\n",
    "\n",
    "def scrape_linkedin(company_name):\n",
    "    linkedin_search_url = f\"https://www.linkedin.com/search/results/content/?keywords={company_name.replace(' ', '%20')}&origin=GLOBAL_SEARCH_HEADER\"\n",
    "    driver.get(linkedin_search_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Extract LinkedIn posts\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    posts = soup.find_all('div', class_='occludable-update')\n",
    "\n",
    "    linkedin_results = []\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_content = post.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "            # Clean up the content\n",
    "            post_content = re.sub(r'\\s+', ' ', post_content)\n",
    "            \n",
    "            # Extract the post link\n",
    "            post_link_tag = post.find('a', href=True)\n",
    "            post_link = post_link_tag['href'] if post_link_tag else \"No link found\"\n",
    "\n",
    "            if post_content:\n",
    "                linkedin_results.append({\n",
    "                    'Company': company_name,\n",
    "                    'Content': post_content,\n",
    "                    'Post Link': post_link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting LinkedIn post: {e}\")\n",
    "\n",
    "    if linkedin_results:\n",
    "        df = pd.DataFrame(linkedin_results)\n",
    "        excel_file_name = \"linkedin_results.xlsx\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(excel_file_name):\n",
    "            # If it exists, append data\n",
    "            with pd.ExcelWriter(excel_file_name, mode='a', if_sheet_exists='new') as writer:\n",
    "                df.to_excel(writer, sheet_name=company_name, index=False)\n",
    "        else:\n",
    "            # If it doesn't exist, create a new file\n",
    "            with pd.ExcelWriter(excel_file_name, mode='w') as writer:\n",
    "                df.to_excel(writer, sheet_name=company_name, index=False)\n",
    "\n",
    "        print(f\"LinkedIn scraping complete for {company_name}. Results saved in '{excel_file_name}'.\")\n",
    "    else:\n",
    "        print(f\"No relevant LinkedIn posts found for {company_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Start the search and extraction process for each company\n",
    "for company in companies_to_search:\n",
    "    print(f\"Searching for: {company}\")\n",
    "    \n",
    "    if scrape_linkedin:\n",
    "        scrape_linkedin(company)\n",
    "    \n",
    "    search_query = f\"{company} news\"\n",
    "    save_to_notepad(f\"Search Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\nQuery: {search_query}\\n{'='*80}\")\n",
    "    extract_links_and_info(search_query)\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "print(f\"Results saved in 'ai_news_results.txt' and 'linkedin_results.csv' if applicable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf604b-b954-4082-beea-58a352ea104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sounddevice numpy scipy SpeechRecognition pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff7e7a61-e369-4b67-9494-1fd5f5498f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Enter to stop.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to stop recording... l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file saved as 'recording_20240820_140210.wav'\n",
      "Transcribing...\n",
      "Minutes saved to 'meeting_minutes.txt'\n",
      "Transcription completed. Results saved in 'meeting_minutes.txt'\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self, fs=44100, channels=1, dtype='int16'):\n",
    "        self.fs = fs\n",
    "        self.channels = channels\n",
    "        self.dtype = dtype\n",
    "        self.recording = False\n",
    "        self.audio_data = []\n",
    "        self.audio_thread = None\n",
    "\n",
    "    def record(self):\n",
    "        self.recording = True\n",
    "        self.audio_data = []\n",
    "        self.audio_thread = threading.Thread(target=self._record_audio)\n",
    "        self.audio_thread.start()\n",
    "\n",
    "    def _record_audio(self):\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels, dtype=self.dtype, callback=self._audio_callback):\n",
    "            while self.recording:\n",
    "                sd.sleep(100)  # Sleep for 100 ms to keep recording\n",
    "\n",
    "    def _audio_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status, flush=True)\n",
    "        if self.recording:\n",
    "            self.audio_data.append(indata.copy())\n",
    "\n",
    "    def stop(self):\n",
    "        self.recording = False\n",
    "        if self.audio_thread:\n",
    "            self.audio_thread.join()\n",
    "\n",
    "    def save_to_wav(self, filename):\n",
    "        if self.audio_data:\n",
    "            audio_data = np.concatenate(self.audio_data, axis=0)\n",
    "            wav.write(filename, self.fs, audio_data)\n",
    "        else:\n",
    "            print(\"No audio data to save.\")\n",
    "\n",
    "def generate_unique_filename(prefix=\"recording\", extension=\".wav\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}_{timestamp}{extension}\"\n",
    "\n",
    "def record_and_transcribe(duration=None, output_file=\"meeting_minutes.txt\", language=\"ar-SA\"):\n",
    "    recognizer = sr.Recognizer()\n",
    "    recorder = AudioRecorder()\n",
    "\n",
    "    print(\"Recording... Press Enter to stop.\")\n",
    "    recorder.record()\n",
    "\n",
    "    try:\n",
    "        input(\"Press Enter to stop recording...\")\n",
    "        recorder.stop()\n",
    "\n",
    "        audio_filename = generate_unique_filename(prefix=\"recording\", extension=\".wav\")\n",
    "        recorder.save_to_wav(audio_filename)\n",
    "\n",
    "        print(f\"Audio file saved as '{audio_filename}'\")\n",
    "\n",
    "        # Convert WAV file to audio file for speech_recognition\n",
    "        with sr.AudioFile(audio_filename) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "        print(\"Transcribing...\")\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data, language=language)\n",
    "            save_to_file(text, output_file)\n",
    "            print(f\"Transcription completed. Results saved in '{output_file}'\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "    finally:\n",
    "        print(\"Exiting...\")\n",
    "\n",
    "def save_to_file(text, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "    print(f\"Minutes saved to '{file_name}'\")\n",
    "\n",
    "def main():\n",
    "    record_and_transcribe(duration=None, output_file=\"meeting_minutes.txt\", language=\"ar-SA\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058e5b2-2a83-405d-b72b-245211109651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
